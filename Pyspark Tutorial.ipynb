{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on PySpark Transformations and MLIB\n",
    "## Learn how to transform  Data and apply Regression techniques using PySpark\n",
    "-  Rossmann Sales Dataset is used in this tutorial and it can be found at https://www.kaggle.com/c/rossmann-store-sales/data .\n",
    "- For installing spark 2.2.1 and  to enable it on your Jupyter Notebook on your local PC (Windows) https://medium.com/@GalarnykMichael/install-spark-on-windows-pyspark-4498a5d8d66c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why PySpark ?\n",
    "Apache Spark is one the most widely used frameworks when it comes to handling and working with Big Data and Python is one of the most widely used programming languages for Data Analysis, Machine Learning, and much more. So, why not use them together? This is where Spark with Python also known as PySpark comes into the picture.\n",
    "\n",
    "PySpark MLlib is a machine-learning library. It is a wrapper over PySpark Core to do data analysis using machine-learning algorithms. It works on distributed systems and is scalable. We can find implementations of classification, clustering, linear regression, and other machine-learning algorithms in PySpark MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a Spark Session and importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import Window\n",
    "spark=SparkSession.builder.appName('Sample').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default no of partitions in spark is 200, it can be changed based on your requirement. We can even repartition the data based on the  columns.\n",
    "- example: dataframe1=dataframe.repartition(x)\n",
    "   - x: can be no of partitions or even the column name  on which you want to partition the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"set spark.sql.shuffle.partitions=10\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes in Spark\n",
    "In Spark, a DataFrame is a distributed collection of rows under named columns. \n",
    "In simple terms, it can be referred as a table in relational database or an Excel sheet with Column headers. \n",
    "It has the following caharacteristics:\n",
    "- Immutable in nature : We can create DataFrame  once but canâ€™t change it. And we can transform a DataFrame after applying transformations.\n",
    "- Lazy Evaluations: Which means that a task is not executed until an action is performed.\n",
    "   - Action commands in spark : count(),collect(), aggregate(),reduce() etc\n",
    "- Distributed: DataFrame are distributed in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading CSV data into Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading the csv files into spark datframes\n",
    "train=spark.read.csv('train_v2.csv',header='true', inferSchema='true')\n",
    "stores=spark.read.csv('store.csv',header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Of the Data retreived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Sales: integer (nullable = true)\n",
      " |-- Customers: integer (nullable = true)\n",
      " |-- Open: integer (nullable = true)\n",
      " |-- Promo: integer (nullable = true)\n",
      " |-- StateHoliday: string (nullable = true)\n",
      " |-- SchoolHoliday: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets have a  look at the schema of the data to get a better unbderstanding of what the data is \n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------------------+-----+---------+----+-----+------------+-------------+\n",
      "|Store|DayOfWeek|               Date|Sales|Customers|Open|Promo|StateHoliday|SchoolHoliday|\n",
      "+-----+---------+-------------------+-----+---------+----+-----+------------+-------------+\n",
      "|    1|        5|2015-01-30 00:00:00| 5577|      616|   1|    1|           0|            0|\n",
      "|    2|        5|2015-01-30 00:00:00| 5919|      624|   1|    1|           0|            0|\n",
      "|    3|        5|2015-01-30 00:00:00| 6911|      678|   1|    1|           0|            0|\n",
      "|    4|        5|2015-01-30 00:00:00|13307|     1632|   1|    1|           0|            0|\n",
      "|    5|        5|2015-01-30 00:00:00| 5640|      617|   1|    1|           0|            0|\n",
      "+-----+---------+-------------------+-----+---------+----+-----+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To view the top 5 records of the dataframe\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+-------------------+-------------------------+------------------------+------+---------------+---------------+---------------+\n",
      "|Store|StoreType|Assortment|CompetitionDistance|CompetitionOpenSinceMonth|CompetitionOpenSinceYear|Promo2|Promo2SinceWeek|Promo2SinceYear|  PromoInterval|\n",
      "+-----+---------+----------+-------------------+-------------------------+------------------------+------+---------------+---------------+---------------+\n",
      "|    1|        c|         a|               1270|                        9|                    2008|     0|           null|           null|           null|\n",
      "|    2|        a|         a|                570|                       11|                    2007|     1|             13|           2010|Jan,Apr,Jul,Oct|\n",
      "|    3|        a|         a|              14130|                       12|                    2006|     1|             14|           2011|Jan,Apr,Jul,Oct|\n",
      "|    4|        c|         c|                620|                        9|                    2009|     0|           null|           null|           null|\n",
      "|    5|        a|         a|              29910|                        4|                    2015|     0|           null|           null|           null|\n",
      "+-----+---------+----------+-------------------+-------------------------+------------------------+------+---------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now let's have a look at the stores data\n",
    "stores.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Date: 2015-01-30 00:00:00\n",
      "Start_Date: 2014-11-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#Extracting the start_date and End_date from the train dataframe\n",
    "print(\"End_Date:\",train.select('Date').rdd.max()[0])\n",
    "print(\"Start_Date:\",train.select('Date').rdd.min()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets compute the average sales per customer per  store \n",
    "# 1.create a new column with avg sales of a customer  for each store and each date\n",
    "train =train.withColumn('SalesPerCustomer',train['Sales']/train['Customers'])\n",
    "# 2.now compute the average customers, sales and sales per customer at doing store level aggregation\n",
    "avg_store=train.groupby('Store').mean('customers','sales','salespercustomer')\n",
    "# 3. join  the results with stores dataframe\n",
    "store=stores.join(avg_store,'Store',how='inner')\n",
    "#store.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Sales: integer (nullable = true)\n",
      " |-- Customers: integer (nullable = true)\n",
      " |-- Open: integer (nullable = true)\n",
      " |-- Promo: integer (nullable = true)\n",
      " |-- StateHoliday: string (nullable = true)\n",
      " |-- SchoolHoliday: integer (nullable = true)\n",
      " |-- SalesPerCustomer: double (nullable = true)\n",
      " |-- StoreType: string (nullable = true)\n",
      " |-- Assortment: string (nullable = true)\n",
      " |-- CompetitionDistance: integer (nullable = true)\n",
      " |-- CompetitionOpenSinceMonth: integer (nullable = true)\n",
      " |-- CompetitionOpenSinceYear: integer (nullable = true)\n",
      " |-- Promo2: integer (nullable = true)\n",
      " |-- Promo2SinceWeek: integer (nullable = true)\n",
      " |-- Promo2SinceYear: integer (nullable = true)\n",
      " |-- PromoInterval: string (nullable = true)\n",
      " |-- avg(customers): double (nullable = true)\n",
      " |-- avg(sales): double (nullable = true)\n",
      " |-- avg(salespercustomer): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#join stores info to the sales(train) data\n",
    "join=train.join(store,'Store',how='inner')\n",
    "join.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----+-----+---------+----+-----+------------+-------------+----------------+---------+----------+-------------------+-------------------------+------------------------+------+---------------+---------------+-------------+--------------+----------+---------------------+\n",
      "|Store|DayOfWeek|Date|Sales|Customers|Open|Promo|StateHoliday|SchoolHoliday|SalesPerCustomer|StoreType|Assortment|CompetitionDistance|CompetitionOpenSinceMonth|CompetitionOpenSinceYear|Promo2|Promo2SinceWeek|Promo2SinceYear|PromoInterval|avg(customers)|avg(sales)|avg(salespercustomer)|\n",
      "+-----+---------+----+-----+---------+----+-----+------------+-------------+----------------+---------+----------+-------------------+-------------------------+------------------------+------+---------------+---------------+-------------+--------------+----------+---------------------+\n",
      "|    0|        0|   0|    0|        0|   0|    0|           0|            0|           12212|        0|         0|                162|                    21312|                   21312|     0|          34928|          34928|        34928|             0|         0|                    0|\n",
      "+-----+---------+----+-----+---------+----+-----+------------+-------------+----------------+---------+----------+-------------------+-------------------------+------------------------+------+---------------+---------------+-------------+--------------+----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now lets find out if there any nulls in the data\n",
    "from pyspark.sql.functions import *\n",
    "join.select([count(when(col(c).isNull(), c)).alias(c) for c in join.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that they are missing values in  few of the columns.\n",
    "Missing Values is one of the most common problem faced during Data Cleaning/ exploratory data analysis.\n",
    "- There are several techniques to handle missing data.\n",
    "  - We can drop the variables or records in  the data if the no of records are less and their impact on the data is less.\n",
    "  - We can also impute them  with mean, median or zero based on the data patterns \n",
    "  - Later we can also use PCA(Principal Component Analysis) or interpolation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For now let fill the columns with null values using '0'\n",
    "join=join.fillna({'PromoInterval':0, 'Promo2SinceYear':0, 'Promo2SinceWeek':0,'CompetitionOpenSinceYear':0,'SalesPerCustomer':0,'CompetitionOpenSinceMonth':0,'StoreType':0,'CompetitionDistance':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. It is fundamental to the application of machine learning and helps in increasing the accuracy of the model. It is really essential in creating the right features.\n",
    "\n",
    "I will be using Random Forest Regressor model for this data.\n",
    "So, I split the date into discrete components so the decision trees were able to make better guesses. \n",
    "I also tried using months since the competition has started and how long the store was in promotion since.\n",
    "\n",
    "Additionally, I also built three  other features - Average sales, customers and sales per customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating new columns by Extracting year, week, month, day from the date column\n",
    "from pyspark.sql.functions import *\n",
    "join=join.withColumn('Day',dayofmonth('date')).withColumn('Month',month('date'))\\\n",
    "     .withColumn('Year',year('date')).withColumn('Week',weekofyear('date'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of months that competition has existed for  and  Number of weeks that promotion has existed for   \n",
    "join=join.withColumn('MonthsCompetitionOpen',12*(join['Year']-join['CompetitionOpenSinceYear'])+(join['Month']-join['CompetitionOpenSinceMonth']))\\\n",
    "     .withColumn('WeeksPromoOpen',12*(join['Year']-join['Promo2SinceYear'])+(join['Week']-join['Promo2SinceWeek']))\n",
    "join=join.withColumn('WeeksPromoOpen',when((col('Promo2SinceYear')==0),0).otherwise(join['WeeksPromoOpen']))\\\n",
    "      .withColumn('MonthsCompetitionOpen',when((col('CompetitionOpenSinceYear')==0),0).otherwise(join['MonthsCompetitionOpen']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let split the data into train and test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join_train = join.filter((join.Date<'2015-01-30'))\n",
    "join_valid = join.filter((join.Date>='2015-01-30'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Categorical and Continous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defing Categorical and Contionous Variables in the dataset\n",
    "cat_vars = ['Store', 'Week', 'Year', 'Month', 'Day', 'DayOfWeek','StateHoliday', 'CompetitionOpenSinceMonth',\n",
    "    'Promo2SinceWeek', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'MonthsCompetitionOpen','WeeksPromoOpen','Promo','SchoolHoliday']\n",
    "\n",
    "contin_vars = ['SalesPerCustomer','Customers' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline\n",
    "A Spark Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage.\n",
    "The Pipeline API, introduced in Spark 1.2, is a high-level API for MLlib. Inspired by the popular implementation in scikit-learn, the concept of Pipelines is to facilitate the creation, tuning, and inspection of practical ML workflows.\n",
    "#### For categorical data lets apply String Indexer \n",
    "String Indexer- Used to convert string columns into numeric.It encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0. String Indexer Functioning is some what similar to Label Encoder from Scikit-Learn.\n",
    "\n",
    "\n",
    "- In the below code, Indexers is pipeline with a series of string Indexers applied on columns that are defined as the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the libraries for string indexing and applying it to categorical values\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import  StringIndexer, VectorAssembler, StandardScaler\n",
    "indexers = [StringIndexer(inputCol=cat_var,outputCol=cat_var+ \"Index4\",handleInvalid=\"skip\") for cat_var in cat_vars]\n",
    "#defining the model features into a variable\n",
    "features_set=[cat_var+ \"Index4\" for cat_var in cat_vars] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler on Continous Values\n",
    "Centering and Scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using the transform method.\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature does not more or less look like standard normally distributed data.\n",
    "\n",
    "- For using the standard scaler in the spark the input data must be in the form of vectors. lets apply vector assembler before passing it into the model.\n",
    "#### VectorAssembler\n",
    "VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cont_assembler = VectorAssembler( inputCols = contin_vars, outputCol = \"contin_test_vars\")\n",
    "contin_scaler = StandardScaler(  inputCol=\"contin_test_vars\", outputCol=\"scaledFeatures\",  withStd=True, withMean=True)\n",
    "features_set.append('scaledFeatures')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing VectorAssembler and creating our Features\n",
    "We must transform our data using the VectorAssembler function to a single column where each row of the DataFrame contains a feature vector. In order to predict, we need to select columns based on which we will then create our features column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembler=VectorAssembler( inputCols = features_set, outputCol = \"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----+-----+---------+----+-----+------------+-------------+----------------+---------+----------+-------------------+-------------------------+------------------------+------+---------------+---------------+-------------+--------------+----------+---------------------+---+-----+----+----+---------------------+--------------+\n",
      "|Store|DayOfWeek|Date|Sales|Customers|Open|Promo|StateHoliday|SchoolHoliday|SalesPerCustomer|StoreType|Assortment|CompetitionDistance|CompetitionOpenSinceMonth|CompetitionOpenSinceYear|Promo2|Promo2SinceWeek|Promo2SinceYear|PromoInterval|avg(customers)|avg(sales)|avg(salespercustomer)|Day|Month|Year|Week|MonthsCompetitionOpen|WeeksPromoOpen|\n",
      "+-----+---------+----+-----+---------+----+-----+------------+-------------+----------------+---------+----------+-------------------+-------------------------+------------------------+------+---------------+---------------+-------------+--------------+----------+---------------------+---+-----+----+----+---------------------+--------------+\n",
      "|    0|        0|   0|    0|        0|   0|    0|           0|            0|               0|        0|         0|                  0|                        0|                       0|     0|              0|              0|            0|             0|         0|                    0|  0|    0|   0|   0|                    0|             0|\n",
      "+-----+---------+----+-----+---------+----+-----+------------+-------------+----------------+---------+----------+-------------------+-------------------------+------------------------+------+---------------+---------------+-------------+--------------+----------+---------------------+---+-----+----+----+---------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Making sure that there are no nulls in the training data\n",
    "from pyspark.sql.functions import *\n",
    "join_train.select([count(when(col(c).isNull(), c)).alias(c) for c in join.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding the above stages to pipeline\n",
    "indexers.append(cont_assembler)\n",
    "indexers.append(contin_scaler)\n",
    "indexers.append(assembler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest \n",
    "Random Forest is an ensembling technique used for classification and regression.  It operates by building a large number of decision trees at training and outputting the predicted value or class of the indiviadual trees in order to reduce the risk of overfitting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StringIndexer_48fe8aeef031f2e0f5e4, StringIndexer_4bfe97418705f84e70f7, StringIndexer_49b6804941edff26ab15, StringIndexer_4f178fb54e16cb99c2b7, StringIndexer_4a2c8ef1b40d90a2ae5a, StringIndexer_4808b93801b11f2fe43f, StringIndexer_4310b1219eb8d07ac995, StringIndexer_4ff0931279fb92e72db9, StringIndexer_4408bfbe956ba33edbc0, StringIndexer_4fc599c1e67378d2244c, StringIndexer_47dda37493c7859637c2, StringIndexer_451bbca50870a3605169, StringIndexer_44c4b226ef28af6b02f3, StringIndexer_4be5ae39397444d05a0b, StringIndexer_42aebec7298b58c9b9bc, StringIndexer_441aa31c286dc6a381d2, StringIndexer_4ec9b9771c1bcf4be0db, StringIndexer_42be9815af3150847198, VectorAssembler_4ef095bfd882c5c1e40e, StandardScaler_4185bdd313b07c3814db, VectorAssembler_4b80b463e3b5089a2ba2, RandomForestRegressor_4392803d0bf90dd9096d]\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries for random forest\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(labelCol=\"Sales\", featuresCol=\"features\", maxBins=1500)\n",
    "# append the model stage in to the indexer\n",
    "indexers.append(rf)\n",
    "#define the indexers as the pipeline\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "# print the stages defined in the pipeline\n",
    "print(indexers)\n",
    "# fit the stages in a pipeline\n",
    "model=pipeline.fit(join_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the values on the test set\n",
    "predictions =model.transform(join_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-----+------------------+\n",
      "|Store|               Date|Sales|        prediction|\n",
      "+-----+-------------------+-----+------------------+\n",
      "|    1|2015-01-30 00:00:00| 5577|5845.1198334143455|\n",
      "|    2|2015-01-30 00:00:00| 5919| 6098.789365477029|\n",
      "|    3|2015-01-30 00:00:00| 6911|6764.8572665876945|\n",
      "|    4|2015-01-30 00:00:00|13307|11466.169721704722|\n",
      "|    5|2015-01-30 00:00:00| 5640| 5923.680647830589|\n",
      "|    6|2015-01-30 00:00:00| 6555| 6288.475040140846|\n",
      "|    7|2015-01-30 00:00:00|11430|10436.872705071195|\n",
      "|    8|2015-01-30 00:00:00| 6401| 6666.074368751535|\n",
      "|    9|2015-01-30 00:00:00| 8072| 6944.267589050096|\n",
      "|   10|2015-01-30 00:00:00| 6350|6341.4473516140115|\n",
      "|   11|2015-01-30 00:00:00|10031| 9542.229182267922|\n",
      "|   12|2015-01-30 00:00:00| 9156| 9085.144163716632|\n",
      "|   13|2015-01-30 00:00:00| 7004| 5939.529759894327|\n",
      "|   14|2015-01-30 00:00:00| 6491| 6453.847984811469|\n",
      "|   15|2015-01-30 00:00:00| 8898| 8345.240343645364|\n",
      "|   16|2015-01-30 00:00:00| 9546|  8972.01898889087|\n",
      "|   17|2015-01-30 00:00:00| 7929| 7230.557641562473|\n",
      "|   18|2015-01-30 00:00:00| 9941| 9059.032145599575|\n",
      "|   19|2015-01-30 00:00:00| 7121|6982.3955741101145|\n",
      "|   20|2015-01-30 00:00:00| 7458| 7170.240284369946|\n",
      "+-----+-------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select the columns to display\n",
    "cols=['Store','Date', 'Sales','prediction']\n",
    "predictions.select(cols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Evaluator\n",
    "Available metrics\n",
    "- Mean Squared Error\n",
    "- Root Mean Squared Error\n",
    "- Mean Absolute Error\n",
    "- Co-efficient of determination(R2)\n",
    "- Explained Variance\n",
    "\n",
    "we will be using root mean sqaure error for our evaluation.\n",
    "\n",
    "RMSE= sqrt(sum(outputvalue-predictedvalue)^2/no of samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Root Mean Square Error  on test data = 1393.67\n"
     ]
    }
   ],
   "source": [
    "# compute the test error\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import mean, min, max,stddev,ceil\n",
    "evaluator = RegressionEvaluator(    labelCol=\"Sales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\" Root Mean Square Error  on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article is to give a basic overview on PySpark and How it can be used for machine learning models.\n",
    "There is still scope for decreasing the error by tuning parameters. Fortunately, Sparkâ€™s MLlib contains a CrossValidator tool that makes tuning hyperparameters a little less painful. The CrossValidator can be used with any algorithm supported by MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
